{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def list2dict(ls):\n",
    "    dic = dict()\n",
    "    for term in ls:\n",
    "        if term not in dic:\n",
    "            dic[term] = 1\n",
    "        else:\n",
    "            dic[term] += 1\n",
    "    return dic\n",
    "\n",
    "def data_formating(raw_df):\n",
    "    df = raw_df\n",
    "    df['Phrase'] = df.Phrase.apply(lambda s: s.lower())\n",
    "    #df = raw_df.assign(PhraseSplit=raw_df.Phrase.apply(lambda s: list2dict(str.split(s, \" \"))))\n",
    "    df = raw_df.assign(PhraseSplit=raw_df.Phrase.apply(lambda s: str.split(str(s), \" \")))\n",
    "    df = df.assign(WordTotal=df.PhraseSplit.apply(lambda l: len(l)), Length=data.Phrase.apply(len))\n",
    "    idx_empty = df[df.Phrase==\" \"].index[0]\n",
    "    df = df.drop(idx_empty)\n",
    "    return df\n",
    "\n",
    "def rvwhitespace(s):\n",
    "    if s==' ' or s=='' :\n",
    "        return ''\n",
    "    if s[0]==' ':\n",
    "        i=1\n",
    "    else:\n",
    "        i=0\n",
    "    if s[-1]==' ':\n",
    "        return s[i:-1]\n",
    "    else:\n",
    "        return s[i:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"train.tsv\", sep=\"\\t\")\n",
    "data = data_formating(data).reset_index().drop('index', 1).sort_values('WordTotal').reset_index().drop('index', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "NV = max(data[data.WordTotal==1].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d = 40\n",
    "P = tf.Variable(tf.random_normal([d, NV]))\n",
    "Ws = tf.Variable(tf.random_normal([5, d]))\n",
    "W  = tf.Variable(tf.random_normal([d, 2*d]))\n",
    "\n",
    "def selectfromSoftmax(p):\n",
    "    return (tf.nn.softmax(tf.transpose(tf.matmul(Ws, p))), tf.argmax(tf.nn.softmax(tf.transpose(tf.matmul(Ws, p))), dimension=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def childPhrase(idx):\n",
    "    numword = data.ix[idx].WordTotal\n",
    "    if numword==1:\n",
    "        return (None, None)\n",
    "    phrase = data.ix[idx].Phrase\n",
    "    phraseSplit = data.ix[idx].PhraseSplit\n",
    "    perhapChild = data[(data.WordTotal < numword)]\n",
    "    childs = perhapChild[perhapChild.Phrase.apply(lambda p: p in phraseSplit)].sort_values('WordTotal', ascending=False)\n",
    "    bigChild = childs.iloc[0]\n",
    "    numchild = bigChild.WordTotal\n",
    "    perhaplittlechild = childs[childs.WordTotal==numword-numchild]\n",
    "\n",
    "    if bigChild.Phrase[0] == phrase[0]:\n",
    "        regex = r\"^\"+ re.escape(bigChild.Phrase)+ r\" (.+)$\"\n",
    "        match = re.search(regex, phrase)\n",
    "        bigFirst = True\n",
    "    else:\n",
    "        regex = r\"^(.+) \"+ re.escape(bigChild.Phrase)+ r\"$\"\n",
    "        match = re.search(regex, phrase)\n",
    "        bigFirst = False\n",
    "\n",
    "    littleChild = childs[childs.Phrase==match.group(1)].iloc[0]\n",
    "\n",
    "    if bigFirst:\n",
    "        return (childs.index[0], childs[childs.Phrase==match.group(1)].index[0])\n",
    "    else:\n",
    "        return (childs[childs.Phrase==match.group(1)].index[0], childs.index[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dic     = {data.Phrase.ix[i]: i for i in data.index}\n",
    "dic_sen = {data.Phrase.ix[i]: data.Sentiment.ix[i] for i in data.index}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def isslicein(sl, s):\n",
    "    # middle\n",
    "    mi = \" \"+sl+\" \" in s\n",
    "    # left\n",
    "    le = (sl+\" \" in s) & (s[0]==sl[0])\n",
    "    # right\n",
    "    ri = (\" \"+sl in s) & (s[-1]==sl[-1])\n",
    "    return mi | le | ri\n",
    "\n",
    "idx=41362\n",
    "s = data.Phrase.ix[idx]\n",
    "slst = str.split(s)\n",
    "numword = len(slst)\n",
    "child = data[(data.WordTotal < numword) & (data.Phrase.apply(lambda st: isslicein(st, s)))].sort_values(['WordTotal', 'Length'], ascending=False)\n",
    "bigChildPhrase = child.Phrase.iloc[0]\n",
    "bigChildIndex  = child.index[0]\n",
    "otherchild = str.split(s, bigChildPhrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PhraseId</th>\n",
       "      <th>SentenceId</th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>PhraseSplit</th>\n",
       "      <th>Length</th>\n",
       "      <th>WordTotal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>156060</td>\n",
       "      <td>8544</td>\n",
       "      <td>chortles</td>\n",
       "      <td>2</td>\n",
       "      <td>[chortles]</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>38206</td>\n",
       "      <td>1817</td>\n",
       "      <td>lushness</td>\n",
       "      <td>3</td>\n",
       "      <td>[lushness]</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38205</td>\n",
       "      <td>1817</td>\n",
       "      <td>opulent</td>\n",
       "      <td>3</td>\n",
       "      <td>[opulent]</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>38199</td>\n",
       "      <td>1816</td>\n",
       "      <td>disassociation</td>\n",
       "      <td>2</td>\n",
       "      <td>[disassociation]</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>38196</td>\n",
       "      <td>1816</td>\n",
       "      <td>specifically</td>\n",
       "      <td>2</td>\n",
       "      <td>[specifically]</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>38192</td>\n",
       "      <td>1816</td>\n",
       "      <td>paints</td>\n",
       "      <td>2</td>\n",
       "      <td>[paints]</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>38188</td>\n",
       "      <td>1816</td>\n",
       "      <td>convincingly</td>\n",
       "      <td>2</td>\n",
       "      <td>[convincingly]</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>140892</td>\n",
       "      <td>7642</td>\n",
       "      <td>94-minute</td>\n",
       "      <td>2</td>\n",
       "      <td>[94-minute]</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>140893</td>\n",
       "      <td>7642</td>\n",
       "      <td>travesty</td>\n",
       "      <td>1</td>\n",
       "      <td>[travesty]</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>38184</td>\n",
       "      <td>1815</td>\n",
       "      <td>cleaner</td>\n",
       "      <td>3</td>\n",
       "      <td>[cleaner]</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>38166</td>\n",
       "      <td>1814</td>\n",
       "      <td>angles</td>\n",
       "      <td>2</td>\n",
       "      <td>[angles]</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>38162</td>\n",
       "      <td>1814</td>\n",
       "      <td>unwieldy</td>\n",
       "      <td>2</td>\n",
       "      <td>[unwieldy]</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>38157</td>\n",
       "      <td>1814</td>\n",
       "      <td>saddled</td>\n",
       "      <td>1</td>\n",
       "      <td>[saddled]</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>140898</td>\n",
       "      <td>7642</td>\n",
       "      <td>unparalleled</td>\n",
       "      <td>3</td>\n",
       "      <td>[unparalleled]</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>38150</td>\n",
       "      <td>1813</td>\n",
       "      <td>accept</td>\n",
       "      <td>2</td>\n",
       "      <td>[accept]</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>38149</td>\n",
       "      <td>1813</td>\n",
       "      <td>enter</td>\n",
       "      <td>3</td>\n",
       "      <td>[enter]</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>140916</td>\n",
       "      <td>7642</td>\n",
       "      <td>mistaken</td>\n",
       "      <td>2</td>\n",
       "      <td>[mistaken]</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>38136</td>\n",
       "      <td>1813</td>\n",
       "      <td>medium</td>\n",
       "      <td>2</td>\n",
       "      <td>[medium]</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>38113</td>\n",
       "      <td>1812</td>\n",
       "      <td>supremes</td>\n",
       "      <td>2</td>\n",
       "      <td>[supremes]</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>38110</td>\n",
       "      <td>1812</td>\n",
       "      <td>gaye</td>\n",
       "      <td>2</td>\n",
       "      <td>[gaye]</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>38109</td>\n",
       "      <td>1812</td>\n",
       "      <td>marvin</td>\n",
       "      <td>2</td>\n",
       "      <td>[marvin]</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>153714</td>\n",
       "      <td>8395</td>\n",
       "      <td>floundering</td>\n",
       "      <td>1</td>\n",
       "      <td>[floundering]</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>38215</td>\n",
       "      <td>1817</td>\n",
       "      <td>match</td>\n",
       "      <td>2</td>\n",
       "      <td>[match]</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>38219</td>\n",
       "      <td>1817</td>\n",
       "      <td>surroundings</td>\n",
       "      <td>2</td>\n",
       "      <td>[surroundings]</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>38229</td>\n",
       "      <td>1818</td>\n",
       "      <td>showgirls</td>\n",
       "      <td>2</td>\n",
       "      <td>[showgirls]</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>38251</td>\n",
       "      <td>1820</td>\n",
       "      <td>exceedingly</td>\n",
       "      <td>3</td>\n",
       "      <td>[exceedingly]</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>38393</td>\n",
       "      <td>1828</td>\n",
       "      <td>flaw</td>\n",
       "      <td>2</td>\n",
       "      <td>[flaw]</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>38392</td>\n",
       "      <td>1828</td>\n",
       "      <td>devastating</td>\n",
       "      <td>0</td>\n",
       "      <td>[devastating]</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>140820</td>\n",
       "      <td>7638</td>\n",
       "      <td>softheaded</td>\n",
       "      <td>2</td>\n",
       "      <td>[softheaded]</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>38381</td>\n",
       "      <td>1827</td>\n",
       "      <td>sentence</td>\n",
       "      <td>2</td>\n",
       "      <td>[sentence]</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156029</th>\n",
       "      <td>11801</td>\n",
       "      <td>509</td>\n",
       "      <td>i stopped thinking about how good it all was ,...</td>\n",
       "      <td>4</td>\n",
       "      <td>[i, stopped, thinking, about, how, good, it, a...</td>\n",
       "      <td>260</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156030</th>\n",
       "      <td>112429</td>\n",
       "      <td>5972</td>\n",
       "      <td>there is something in full frontal , i guess ,...</td>\n",
       "      <td>1</td>\n",
       "      <td>[there, is, something, in, full, frontal, ,, i...</td>\n",
       "      <td>240</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156031</th>\n",
       "      <td>136947</td>\n",
       "      <td>7408</td>\n",
       "      <td>it 's too harsh to work as a piece of storytel...</td>\n",
       "      <td>2</td>\n",
       "      <td>[it, 's, too, harsh, to, work, as, a, piece, o...</td>\n",
       "      <td>244</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156032</th>\n",
       "      <td>82376</td>\n",
       "      <td>4255</td>\n",
       "      <td>even if the enticing prospect of a lot of nubi...</td>\n",
       "      <td>0</td>\n",
       "      <td>[even, if, the, enticing, prospect, of, a, lot...</td>\n",
       "      <td>260</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156033</th>\n",
       "      <td>44287</td>\n",
       "      <td>2148</td>\n",
       "      <td>the movie should jolt you out of your seat a c...</td>\n",
       "      <td>3</td>\n",
       "      <td>[the, movie, should, jolt, you, out, of, your,...</td>\n",
       "      <td>210</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156034</th>\n",
       "      <td>9651</td>\n",
       "      <td>403</td>\n",
       "      <td>notwithstanding my problem with the movie 's f...</td>\n",
       "      <td>3</td>\n",
       "      <td>[notwithstanding, my, problem, with, the, movi...</td>\n",
       "      <td>261</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156035</th>\n",
       "      <td>131267</td>\n",
       "      <td>7075</td>\n",
       "      <td>it 's hard to imagine anybody ever being `` in...</td>\n",
       "      <td>3</td>\n",
       "      <td>[it, 's, hard, to, imagine, anybody, ever, bei...</td>\n",
       "      <td>232</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156036</th>\n",
       "      <td>135699</td>\n",
       "      <td>7333</td>\n",
       "      <td>may not be as cutting , as witty or as true as...</td>\n",
       "      <td>2</td>\n",
       "      <td>[may, not, be, as, cutting, ,, as, witty, or, ...</td>\n",
       "      <td>237</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156037</th>\n",
       "      <td>80458</td>\n",
       "      <td>4145</td>\n",
       "      <td>if s&amp;m seems like a strange route to true love...</td>\n",
       "      <td>3</td>\n",
       "      <td>[if, s&amp;m, seems, like, a, strange, route, to, ...</td>\n",
       "      <td>227</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156038</th>\n",
       "      <td>145400</td>\n",
       "      <td>7901</td>\n",
       "      <td>-lrb- clooney 's -rrb- debut can be accused of...</td>\n",
       "      <td>3</td>\n",
       "      <td>[-lrb-, clooney, 's, -rrb-, debut, can, be, ac...</td>\n",
       "      <td>268</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156039</th>\n",
       "      <td>35602</td>\n",
       "      <td>1678</td>\n",
       "      <td>to the core of what it actually means to face ...</td>\n",
       "      <td>3</td>\n",
       "      <td>[to, the, core, of, what, it, actually, means,...</td>\n",
       "      <td>208</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156040</th>\n",
       "      <td>136946</td>\n",
       "      <td>7408</td>\n",
       "      <td>it 's too harsh to work as a piece of storytel...</td>\n",
       "      <td>3</td>\n",
       "      <td>[it, 's, too, harsh, to, work, as, a, piece, o...</td>\n",
       "      <td>246</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156041</th>\n",
       "      <td>142012</td>\n",
       "      <td>7705</td>\n",
       "      <td>sitting in the third row of the imax cinema at...</td>\n",
       "      <td>3</td>\n",
       "      <td>[sitting, in, the, third, row, of, the, imax, ...</td>\n",
       "      <td>259</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156042</th>\n",
       "      <td>135698</td>\n",
       "      <td>7333</td>\n",
       "      <td>may not be as cutting , as witty or as true as...</td>\n",
       "      <td>3</td>\n",
       "      <td>[may, not, be, as, cutting, ,, as, witty, or, ...</td>\n",
       "      <td>239</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156043</th>\n",
       "      <td>71219</td>\n",
       "      <td>3635</td>\n",
       "      <td>the most memorable moment was when green threw...</td>\n",
       "      <td>0</td>\n",
       "      <td>[the, most, memorable, moment, was, when, gree...</td>\n",
       "      <td>248</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156044</th>\n",
       "      <td>18580</td>\n",
       "      <td>817</td>\n",
       "      <td>if you are curious to see the darker side of w...</td>\n",
       "      <td>1</td>\n",
       "      <td>[if, you, are, curious, to, see, the, darker, ...</td>\n",
       "      <td>256</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156045</th>\n",
       "      <td>35601</td>\n",
       "      <td>1678</td>\n",
       "      <td>cuts to the core of what it actually means to ...</td>\n",
       "      <td>3</td>\n",
       "      <td>[cuts, to, the, core, of, what, it, actually, ...</td>\n",
       "      <td>213</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156046</th>\n",
       "      <td>59651</td>\n",
       "      <td>3010</td>\n",
       "      <td>it 's a bad sign when you 're rooting for the ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[it, 's, a, bad, sign, when, you, 're, rooting...</td>\n",
       "      <td>248</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156047</th>\n",
       "      <td>47640</td>\n",
       "      <td>2323</td>\n",
       "      <td>when it 's not wallowing in hormonal melodrama...</td>\n",
       "      <td>3</td>\n",
       "      <td>[when, it, 's, not, wallowing, in, hormonal, m...</td>\n",
       "      <td>254</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156048</th>\n",
       "      <td>43803</td>\n",
       "      <td>2124</td>\n",
       "      <td>-lrb- city -rrb- reminds us how realistically ...</td>\n",
       "      <td>3</td>\n",
       "      <td>[-lrb-, city, -rrb-, reminds, us, how, realist...</td>\n",
       "      <td>279</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156049</th>\n",
       "      <td>75359</td>\n",
       "      <td>3866</td>\n",
       "      <td>build some robots , haul 'em to the theatre wi...</td>\n",
       "      <td>0</td>\n",
       "      <td>[build, some, robots, ,, haul, 'em, to, the, t...</td>\n",
       "      <td>249</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156050</th>\n",
       "      <td>35600</td>\n",
       "      <td>1678</td>\n",
       "      <td>cuts to the core of what it actually means to ...</td>\n",
       "      <td>4</td>\n",
       "      <td>[cuts, to, the, core, of, what, it, actually, ...</td>\n",
       "      <td>215</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156051</th>\n",
       "      <td>105156</td>\n",
       "      <td>5555</td>\n",
       "      <td>... spiced with humor -lrb- ' i speak fluent f...</td>\n",
       "      <td>3</td>\n",
       "      <td>[..., spiced, with, humor, -lrb-, ', i, speak,...</td>\n",
       "      <td>283</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156052</th>\n",
       "      <td>59650</td>\n",
       "      <td>3010</td>\n",
       "      <td>it 's a bad sign when you 're rooting for the ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[it, 's, a, bad, sign, when, you, 're, rooting...</td>\n",
       "      <td>250</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156053</th>\n",
       "      <td>87837</td>\n",
       "      <td>4563</td>\n",
       "      <td>there are n't too many films that can be as si...</td>\n",
       "      <td>4</td>\n",
       "      <td>[there, are, n't, too, many, films, that, can,...</td>\n",
       "      <td>257</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156054</th>\n",
       "      <td>135697</td>\n",
       "      <td>7333</td>\n",
       "      <td>it may not be as cutting , as witty or as true...</td>\n",
       "      <td>2</td>\n",
       "      <td>[it, may, not, be, as, cutting, ,, as, witty, ...</td>\n",
       "      <td>242</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156055</th>\n",
       "      <td>18579</td>\n",
       "      <td>817</td>\n",
       "      <td>if you are curious to see the darker side of w...</td>\n",
       "      <td>1</td>\n",
       "      <td>[if, you, are, curious, to, see, the, darker, ...</td>\n",
       "      <td>258</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156056</th>\n",
       "      <td>75358</td>\n",
       "      <td>3866</td>\n",
       "      <td>build some robots , haul 'em to the theatre wi...</td>\n",
       "      <td>0</td>\n",
       "      <td>[build, some, robots, ,, haul, 'em, to, the, t...</td>\n",
       "      <td>251</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156057</th>\n",
       "      <td>87836</td>\n",
       "      <td>4563</td>\n",
       "      <td>there are n't too many films that can be as si...</td>\n",
       "      <td>4</td>\n",
       "      <td>[there, are, n't, too, many, films, that, can,...</td>\n",
       "      <td>259</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156058</th>\n",
       "      <td>35599</td>\n",
       "      <td>1678</td>\n",
       "      <td>it cuts to the core of what it actually means ...</td>\n",
       "      <td>4</td>\n",
       "      <td>[it, cuts, to, the, core, of, what, it, actual...</td>\n",
       "      <td>218</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>156059 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        PhraseId  SentenceId  \\\n",
       "0         156060        8544   \n",
       "1          38206        1817   \n",
       "2          38205        1817   \n",
       "3          38199        1816   \n",
       "4          38196        1816   \n",
       "5          38192        1816   \n",
       "6          38188        1816   \n",
       "7         140892        7642   \n",
       "8         140893        7642   \n",
       "9          38184        1815   \n",
       "10         38166        1814   \n",
       "11         38162        1814   \n",
       "12         38157        1814   \n",
       "13        140898        7642   \n",
       "14         38150        1813   \n",
       "15         38149        1813   \n",
       "16        140916        7642   \n",
       "17         38136        1813   \n",
       "18         38113        1812   \n",
       "19         38110        1812   \n",
       "20         38109        1812   \n",
       "21        153714        8395   \n",
       "22         38215        1817   \n",
       "23         38219        1817   \n",
       "24         38229        1818   \n",
       "25         38251        1820   \n",
       "26         38393        1828   \n",
       "27         38392        1828   \n",
       "28        140820        7638   \n",
       "29         38381        1827   \n",
       "...          ...         ...   \n",
       "156029     11801         509   \n",
       "156030    112429        5972   \n",
       "156031    136947        7408   \n",
       "156032     82376        4255   \n",
       "156033     44287        2148   \n",
       "156034      9651         403   \n",
       "156035    131267        7075   \n",
       "156036    135699        7333   \n",
       "156037     80458        4145   \n",
       "156038    145400        7901   \n",
       "156039     35602        1678   \n",
       "156040    136946        7408   \n",
       "156041    142012        7705   \n",
       "156042    135698        7333   \n",
       "156043     71219        3635   \n",
       "156044     18580         817   \n",
       "156045     35601        1678   \n",
       "156046     59651        3010   \n",
       "156047     47640        2323   \n",
       "156048     43803        2124   \n",
       "156049     75359        3866   \n",
       "156050     35600        1678   \n",
       "156051    105156        5555   \n",
       "156052     59650        3010   \n",
       "156053     87837        4563   \n",
       "156054    135697        7333   \n",
       "156055     18579         817   \n",
       "156056     75358        3866   \n",
       "156057     87836        4563   \n",
       "156058     35599        1678   \n",
       "\n",
       "                                                   Phrase  Sentiment  \\\n",
       "0                                                chortles          2   \n",
       "1                                                lushness          3   \n",
       "2                                                 opulent          3   \n",
       "3                                          disassociation          2   \n",
       "4                                            specifically          2   \n",
       "5                                                  paints          2   \n",
       "6                                            convincingly          2   \n",
       "7                                               94-minute          2   \n",
       "8                                                travesty          1   \n",
       "9                                                 cleaner          3   \n",
       "10                                                 angles          2   \n",
       "11                                               unwieldy          2   \n",
       "12                                                saddled          1   \n",
       "13                                           unparalleled          3   \n",
       "14                                                 accept          2   \n",
       "15                                                  enter          3   \n",
       "16                                               mistaken          2   \n",
       "17                                                 medium          2   \n",
       "18                                               supremes          2   \n",
       "19                                                   gaye          2   \n",
       "20                                                 marvin          2   \n",
       "21                                            floundering          1   \n",
       "22                                                  match          2   \n",
       "23                                           surroundings          2   \n",
       "24                                              showgirls          2   \n",
       "25                                            exceedingly          3   \n",
       "26                                                   flaw          2   \n",
       "27                                            devastating          0   \n",
       "28                                             softheaded          2   \n",
       "29                                               sentence          2   \n",
       "...                                                   ...        ...   \n",
       "156029  i stopped thinking about how good it all was ,...          4   \n",
       "156030  there is something in full frontal , i guess ,...          1   \n",
       "156031  it 's too harsh to work as a piece of storytel...          2   \n",
       "156032  even if the enticing prospect of a lot of nubi...          0   \n",
       "156033  the movie should jolt you out of your seat a c...          3   \n",
       "156034  notwithstanding my problem with the movie 's f...          3   \n",
       "156035  it 's hard to imagine anybody ever being `` in...          3   \n",
       "156036  may not be as cutting , as witty or as true as...          2   \n",
       "156037  if s&m seems like a strange route to true love...          3   \n",
       "156038  -lrb- clooney 's -rrb- debut can be accused of...          3   \n",
       "156039  to the core of what it actually means to face ...          3   \n",
       "156040  it 's too harsh to work as a piece of storytel...          3   \n",
       "156041  sitting in the third row of the imax cinema at...          3   \n",
       "156042  may not be as cutting , as witty or as true as...          3   \n",
       "156043  the most memorable moment was when green threw...          0   \n",
       "156044  if you are curious to see the darker side of w...          1   \n",
       "156045  cuts to the core of what it actually means to ...          3   \n",
       "156046  it 's a bad sign when you 're rooting for the ...          0   \n",
       "156047  when it 's not wallowing in hormonal melodrama...          3   \n",
       "156048  -lrb- city -rrb- reminds us how realistically ...          3   \n",
       "156049  build some robots , haul 'em to the theatre wi...          0   \n",
       "156050  cuts to the core of what it actually means to ...          4   \n",
       "156051  ... spiced with humor -lrb- ' i speak fluent f...          3   \n",
       "156052  it 's a bad sign when you 're rooting for the ...          0   \n",
       "156053  there are n't too many films that can be as si...          4   \n",
       "156054  it may not be as cutting , as witty or as true...          2   \n",
       "156055  if you are curious to see the darker side of w...          1   \n",
       "156056  build some robots , haul 'em to the theatre wi...          0   \n",
       "156057  there are n't too many films that can be as si...          4   \n",
       "156058  it cuts to the core of what it actually means ...          4   \n",
       "\n",
       "                                              PhraseSplit  Length  WordTotal  \n",
       "0                                              [chortles]       8          1  \n",
       "1                                              [lushness]       8          1  \n",
       "2                                               [opulent]       7          1  \n",
       "3                                        [disassociation]      14          1  \n",
       "4                                          [specifically]      12          1  \n",
       "5                                                [paints]       6          1  \n",
       "6                                          [convincingly]      12          1  \n",
       "7                                             [94-minute]       9          1  \n",
       "8                                              [travesty]       8          1  \n",
       "9                                               [cleaner]       7          1  \n",
       "10                                               [angles]       6          1  \n",
       "11                                             [unwieldy]       8          1  \n",
       "12                                              [saddled]       7          1  \n",
       "13                                         [unparalleled]      12          1  \n",
       "14                                               [accept]       6          1  \n",
       "15                                                [enter]       5          1  \n",
       "16                                             [mistaken]       8          1  \n",
       "17                                               [medium]       6          1  \n",
       "18                                             [supremes]       8          1  \n",
       "19                                                 [gaye]       4          1  \n",
       "20                                               [marvin]       6          1  \n",
       "21                                          [floundering]      11          1  \n",
       "22                                                [match]       5          1  \n",
       "23                                         [surroundings]      12          1  \n",
       "24                                            [showgirls]       9          1  \n",
       "25                                          [exceedingly]      11          1  \n",
       "26                                                 [flaw]       4          1  \n",
       "27                                          [devastating]      11          1  \n",
       "28                                           [softheaded]      10          1  \n",
       "29                                             [sentence]       8          1  \n",
       "...                                                   ...     ...        ...  \n",
       "156029  [i, stopped, thinking, about, how, good, it, a...     260         48  \n",
       "156030  [there, is, something, in, full, frontal, ,, i...     240         48  \n",
       "156031  [it, 's, too, harsh, to, work, as, a, piece, o...     244         48  \n",
       "156032  [even, if, the, enticing, prospect, of, a, lot...     260         49  \n",
       "156033  [the, movie, should, jolt, you, out, of, your,...     210         49  \n",
       "156034  [notwithstanding, my, problem, with, the, movi...     261         49  \n",
       "156035  [it, 's, hard, to, imagine, anybody, ever, bei...     232         49  \n",
       "156036  [may, not, be, as, cutting, ,, as, witty, or, ...     237         49  \n",
       "156037  [if, s&m, seems, like, a, strange, route, to, ...     227         49  \n",
       "156038  [-lrb-, clooney, 's, -rrb-, debut, can, be, ac...     268         49  \n",
       "156039  [to, the, core, of, what, it, actually, means,...     208         49  \n",
       "156040  [it, 's, too, harsh, to, work, as, a, piece, o...     246         49  \n",
       "156041  [sitting, in, the, third, row, of, the, imax, ...     259         50  \n",
       "156042  [may, not, be, as, cutting, ,, as, witty, or, ...     239         50  \n",
       "156043  [the, most, memorable, moment, was, when, gree...     248         50  \n",
       "156044  [if, you, are, curious, to, see, the, darker, ...     256         50  \n",
       "156045  [cuts, to, the, core, of, what, it, actually, ...     213         50  \n",
       "156046  [it, 's, a, bad, sign, when, you, 're, rooting...     248         50  \n",
       "156047  [when, it, 's, not, wallowing, in, hormonal, m...     254         50  \n",
       "156048  [-lrb-, city, -rrb-, reminds, us, how, realist...     279         50  \n",
       "156049  [build, some, robots, ,, haul, 'em, to, the, t...     249         51  \n",
       "156050  [cuts, to, the, core, of, what, it, actually, ...     215         51  \n",
       "156051  [..., spiced, with, humor, -lrb-, ', i, speak,...     283         51  \n",
       "156052  [it, 's, a, bad, sign, when, you, 're, rooting...     250         51  \n",
       "156053  [there, are, n't, too, many, films, that, can,...     257         51  \n",
       "156054  [it, may, not, be, as, cutting, ,, as, witty, ...     242         51  \n",
       "156055  [if, you, are, curious, to, see, the, darker, ...     258         51  \n",
       "156056  [build, some, robots, ,, haul, 'em, to, the, t...     251         52  \n",
       "156057  [there, are, n't, too, many, films, that, can,...     259         52  \n",
       "156058  [it, cuts, to, the, core, of, what, it, actual...     218         52  \n",
       "\n",
       "[156059 rows x 7 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def isslicein(sl, s):\n",
    "    # middle\n",
    "    mi = \" \"+sl+\" \" in s\n",
    "    # left\n",
    "    le = (sl+\" \" in s) & (s[0]==sl[0])\n",
    "    # right\n",
    "    ri = (\" \"+sl in s) & (s[-1]==sl[-1])\n",
    "    return mi | le | ri\n",
    "\n",
    "def child(idx):\n",
    "    s = data.Phrase.ix[idx]\n",
    "    slst = str.split(s)\n",
    "    numword = len(slst)\n",
    "    if numword==1:\n",
    "        return [idx]\n",
    "    child = data[(data.WordTotal < numword) & (data.Phrase.apply(lambda st: isslicein(st, s)))].sort_values(['WordTotal', 'Length'], ascending=False)\n",
    "    bigChildPhrase = child.Phrase.iloc[0]\n",
    "    bigChildIndex  = child.index[0]\n",
    "    otherchild = str.split(s, bigChildPhrase)\n",
    "    \n",
    "    if otherchild[0]=='':\n",
    "        bigChildP   = prob(bigChildPhrase)\n",
    "        otherchildP = prob(rvwhitespace(otherchild[1]))\n",
    "        p = inference(bigChildP, otherchildP)\n",
    "    elif otherchild[-1]=='':\n",
    "        bigChildP   = prob(bigChildPhrase)\n",
    "        otherchildP = prob(rvwhitespace(otherchild[0]))\n",
    "        p = inference(otherchildP, bigChildP)\n",
    "    else:\n",
    "        bigChildP   = prob(bigChildPhrase)\n",
    "        prevP       = prob(rvwhitespace(otherchild[0]))\n",
    "        laterP      = prob(rvwhitespace(otherchild[-1]))\n",
    "        p = inference( prevP, inference(bigChildP, laterP) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "init = tf.initialize_all_variables()\n",
    "sess = tf.Session()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dic     = {data.Phrase.ix[i]: i for i in data.index}\n",
    "dic_sen = {data.Phrase.ix[i]: data.Sentiment.ix[i] for i in data.index}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "slice index 16530 of dimension 1 out of bounds.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-709acef08609>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0mxs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPhrase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_sum\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mxs\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-26-709acef08609>\u001b[0m in \u001b[0;36mcost\u001b[0;34m(s)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0ms_vec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselectfromSoftmax\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mprob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m     \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdic_sen\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0ms_vec\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-26-709acef08609>\u001b[0m in \u001b[0;36mprob\u001b[0;34m(s)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mnumword\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnumword\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mP\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdic\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mchild\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWordTotal\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mnumword\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPhrase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mst\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0misslicein\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'WordTotal'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Length'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mascending\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/array_ops.pyc\u001b[0m in \u001b[0;36m_SliceHelperVar\u001b[0;34m(var, slice_spec)\u001b[0m\n\u001b[1;32m    604\u001b[0m   \"\"\"\n\u001b[1;32m    605\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 606\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_SliceHelper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_AsTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslice_spec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    607\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    608\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_override_operator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"__getitem__\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_SliceHelper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/array_ops.pyc\u001b[0m in \u001b[0;36m_SliceHelper\u001b[0;34m(tensor, slice_spec, var)\u001b[0m\n\u001b[1;32m    381\u001b[0m         \u001b[0mellipsis_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mellipsis_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m         \u001b[0mvar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 383\u001b[0;31m         name=name)\n\u001b[0m\u001b[1;32m    384\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/array_ops.pyc\u001b[0m in \u001b[0;36mstrided_slice\u001b[0;34m(input_, begin, end, strides, begin_mask, end_mask, ellipsis_mask, new_axis_mask, shrink_axis_mask, var, name)\u001b[0m\n\u001b[1;32m    535\u001b[0m       \u001b[0mellipsis_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mellipsis_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    536\u001b[0m       \u001b[0mnew_axis_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnew_axis_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 537\u001b[0;31m       shrink_axis_mask=shrink_axis_mask)\n\u001b[0m\u001b[1;32m    538\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    539\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0massign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/gen_array_ops.pyc\u001b[0m in \u001b[0;36mstrided_slice\u001b[0;34m(input, begin, end, strides, begin_mask, end_mask, ellipsis_mask, new_axis_mask, shrink_axis_mask, name)\u001b[0m\n\u001b[1;32m   2748\u001b[0m                                 \u001b[0mellipsis_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mellipsis_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2749\u001b[0m                                 \u001b[0mnew_axis_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnew_axis_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2750\u001b[0;31m                                 shrink_axis_mask=shrink_axis_mask, name=name)\n\u001b[0m\u001b[1;32m   2751\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2752\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.pyc\u001b[0m in \u001b[0;36mapply_op\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    747\u001b[0m           op = g.create_op(op_type_name, inputs, output_types, name=scope,\n\u001b[1;32m    748\u001b[0m                            \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattr_protos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 749\u001b[0;31m                            op_def=op_def)\n\u001b[0m\u001b[1;32m    750\u001b[0m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    751\u001b[0m           return _Restructure(ops.convert_n_to_tensor(outputs),\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36mcreate_op\u001b[0;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_shapes, compute_device)\u001b[0m\n\u001b[1;32m   2380\u001b[0m                     original_op=self._default_original_op, op_def=op_def)\n\u001b[1;32m   2381\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcompute_shapes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2382\u001b[0;31m       \u001b[0mset_shapes_for_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2383\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2384\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_record_op_seen_by_control_dependencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36mset_shapes_for_outputs\u001b[0;34m(op)\u001b[0m\n\u001b[1;32m   1781\u001b[0m       raise RuntimeError(\"No shape function registered for standard op: %s\"\n\u001b[1;32m   1782\u001b[0m                          % op.type)\n\u001b[0;32m-> 1783\u001b[0;31m   \u001b[0mshapes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshape_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1784\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mshapes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m     raise RuntimeError(\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/array_ops.pyc\u001b[0m in \u001b[0;36m_DelegateStridedSliceShape\u001b[0;34m(op)\u001b[0m\n\u001b[1;32m   1643\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRegisterShape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"StridedSlice\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1644\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_DelegateStridedSliceShape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1645\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mcommon_shapes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_cpp_shape_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_tensors_needed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1646\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1647\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/tensorflow/python/framework/common_shapes.pyc\u001b[0m in \u001b[0;36mcall_cpp_shape_fn\u001b[0;34m(op, input_tensors_needed, debug_python_shape_fn)\u001b[0m\n\u001b[1;32m    594\u001b[0m                                                              status)\n\u001b[1;32m    595\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 596\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    597\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m   \u001b[0;31m# Convert TensorShapeProto values in output_shapes.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: slice index 16530 of dimension 1 out of bounds."
     ]
    }
   ],
   "source": [
    "def isslicein(sl, s):\n",
    "    # middle\n",
    "    mi = \" \"+sl+\" \" in s\n",
    "    # left\n",
    "    le = (sl+\" \" in s) & (s[0]==sl[0])\n",
    "    # right\n",
    "    ri = (\" \"+sl in s) & (s[-1]==sl[-1])\n",
    "    return mi | le | ri\n",
    "\n",
    "def inference(firstP, secondP):\n",
    "    return tf.nn.tanh( tf.matmul( W, tf.concat(0, [firstP, secondP]) ) )\n",
    "\n",
    "def prob(s):\n",
    "    slst = str.split(s)\n",
    "    numword = len(slst)\n",
    "    if numword==1:\n",
    "        return tf.transpose([P[:,dic[s]]])\n",
    "    \n",
    "    child = data[(data.WordTotal < numword) & (data.Phrase.apply(lambda st: isslicein(st, s)))].sort_values(['WordTotal', 'Length'], ascending=False)\n",
    "    bigChildPhrase = child.Phrase.iloc[0]\n",
    "    otherchild = str.split(s, bigChildPhrase)\n",
    "    \n",
    "    if otherchild[0]=='':\n",
    "        bigChildP   = prob(bigChildPhrase)\n",
    "        otherchildP = prob(rvwhitespace(otherchild[1]))\n",
    "        p = inference(bigChildP, otherchildP)\n",
    "    elif otherchild[-1]=='':\n",
    "        bigChildP   = prob(bigChildPhrase)\n",
    "        otherchildP = prob(rvwhitespace(otherchild[0]))\n",
    "        p = inference(otherchildP, bigChildP)\n",
    "    else:\n",
    "        bigChildP   = prob(bigChildPhrase)\n",
    "        prevP       = prob(rvwhitespace(otherchild[0]))\n",
    "        laterP      = prob(rvwhitespace(otherchild[-1]))\n",
    "        p = inference( prevP, inference(bigChildP, laterP) )\n",
    "    #if s in dic.keys():\n",
    "        #p_op.append(P[:, dic[s]].assign(p[:,0]))\n",
    "    return p\n",
    "\n",
    "def cost(s):\n",
    "    s_vec, k = selectfromSoftmax( prob(s) )\n",
    "    k = dic_sen[s]\n",
    "    return - tf.log( s_vec[0,k] )\n",
    "\n",
    "xs = data.Phrase.sample(10000).get_values()\n",
    "\n",
    "loss = tf.reduce_sum( [cost(s) for s in xs] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 測試區塊\n",
    "s = \"trenchant ,\"\n",
    "slst = str.split(s)\n",
    "numword = len(slst)\n",
    "child = data[(data.WordTotal < numword) & (data.Phrase.apply(lambda st: isslicein(st, s)))].sort_values('WordTotal', ascending=False)\n",
    "bigChildPhrase = child.Phrase.iloc[0]\n",
    "otherchild = str.split(s, bigChildPhrase)\n",
    "\n",
    "if otherchild[0]=='':\n",
    "    bigChildP   = prob(bigChildPhrase)\n",
    "    otherchildP = prob(rvwhitespace(otherchild[1]))\n",
    "    p = tf.nn.tanh( tf.matmul( W, tf.concat(0, [bigChildP, otherchildP]) ) )\n",
    "elif otherchild[-1]=='':\n",
    "    bigChildP   = prob(bigChildPhrase)\n",
    "    otherchildP = prob(rvwhitespace(otherchild[0]))\n",
    "    p = tf.nn.tanh( tf.matmul( W, tf.concat(0, [otherchildP, bigChildP]) ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for loop in range(1000):\n",
    "    xs = data.Phrase.sample(20).get_values()\n",
    "    loss = tf.reduce_sum( [cost(s) for s in xs] )\n",
    "    train = tf.train.GradientDescentOptimizer(0.1).minimize(loss)\n",
    "\n",
    "    sess.run(train)\n",
    "    print(sess.run( loss ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sess.close()\n",
    "t = tf.train.GradientDescentOptimizer(0.0001)\n",
    "init = tf.initialize_all_variables()\n",
    "sess = tf.Session()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.assign(lens=data.Phrase.apply(len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p_op=[]\n",
    "\n",
    "for s in data.Phrase:\n",
    "    sess.run( prob(s))\n",
    "for op in p_op:\n",
    "    sess.run(op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for s in data.Phrase:\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.random.randint?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
